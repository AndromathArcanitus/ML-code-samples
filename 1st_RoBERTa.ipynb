{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYHvDvrT/wtmaR4Hmz2WuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndromathArcanitus/ML-code-samples/blob/main/1st_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "R4qSsTRfwAqC",
        "outputId": "4a30fac2-1519-4f8f-fec8-aa6ab5bc9110"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# We won't need TensorFlow here\\n!pip uninstall -y tensorflow\\n# Install `transformers` from master\\n!pip install git+https://github.com/huggingface/transformers\\n!pip list | grep -E 'transformers|tokenizers'\\n# transformers version at notebook update --- 2.9.1\\n# tokenizers version at notebook update --- 0.7.0\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#@title Step 2:APRIL 2023 UPDATE: Installing Hugging Face Transformers\n",
        "'''\n",
        "# We won't need TensorFlow here\n",
        "!pip uninstall -y tensorflow\n",
        "# Install `transformers` from master\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'\n",
        "# transformers version at notebook update --- 2.9.1\n",
        "# tokenizers version at notebook update --- 0.7.0\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjCWoWg5wbL5",
        "outputId": "2ccb154a-78e1-4a29-d4d0-4159409ace87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3: Training a Tokenizer\n",
        "%%time \n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojt4uPadwdjn",
        "outputId": "b9bbb772-215c-4020-d291-68ebac7ab220"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.56 s, sys: 135 ms, total: 4.7 s\n",
            "Wall time: 2.74 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 4: Saving the files to disk\n",
        "import os\n",
        "token_dir = '/content/KantaiBERT'\n",
        "if not os.path.exists(token_dir):\n",
        "    os.makedirs(token_dir)\n",
        "tokenizer.save_model('KantaiBERT')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkrP1M48ww9O",
        "outputId": "eb3edf57-ec42-4cfa-8d20-ec684f5cac4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 5 Loading the Trained Tokenizer Files \n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./KantaiBERT/vocab.json\",\n",
        "    \"./KantaiBERT/merges.txt\",\n",
        ")"
      ],
      "metadata": {
        "id": "ylL-4N8sxcz0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)"
      ],
      "metadata": {
        "id": "CoTak-QaxoV0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 6: Checking Resource Constraints: GPU and NVIDIA\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyoA6ffJyB08",
        "outputId": "f0507808-10ed-4b2e-d276-6ebd51cc35d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  1 01:36:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Checking that PyTorch Sees CUDA\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jnbr5vOwyBvk",
        "outputId": "533aa288-6c55-49cb-ae46-ae34568ed7ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 7: Defining the configuration of the Model\n",
        "from transformers import RobertaConfig\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "metadata": {
        "id": "Kfq_jrL_yMPI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP9CB9otyoFC",
        "outputId": "e82e9ddf-ca04-4e02-8d94-d068aa18f935"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 52000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 8: Re-creating the Tokenizer in Transformers\n",
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512)"
      ],
      "metadata": {
        "id": "5BiIP898ytGe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 9: Initializing a Model From Scratch\n",
        "from transformers import RobertaForMaskedLM"
      ],
      "metadata": {
        "id": "R1QqN5rsytEf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForMaskedLM(config=config)"
      ],
      "metadata": {
        "id": "IK37_2byytCs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 10: Building the Dataset\n",
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./kant.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SgKT7aWys7m",
        "outputId": "f36c6ece-5d68-4e3a-c14e-64ce16e7477d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.6 s, sys: 952 ms, total: 35.5 s\n",
            "Wall time: 43.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 11: Defining a Data Collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "qyi9p4dVys5Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 11: Defining a Data Collator\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "metadata": {
        "id": "ddVy2gXoys25"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhjlKxJuj0nl",
        "outputId": "6ba6859a-9b48-4775-c6a6-c1cb6c679bc8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 12: Initializing the Trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./KantaiBERT\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "vxvVEBNnys0a"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 13: Pre-training the Model\n",
        "%%time\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WaCWyV04ysyR",
        "outputId": "b5ca0c4a-530a-4b24-e975-133063c08ff3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2857' max='2857' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2857/2857 09:56, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>6.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.716100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.254100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.985400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>4.814200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9min 42s, sys: 2.21 s, total: 9min 44s\n",
            "Wall time: 9min 59s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2857, training_loss=5.386596337873534, metrics={'train_runtime': 599.0931, 'train_samples_per_second': 305.173, 'train_steps_per_second': 4.769, 'total_flos': 934691298792576.0, 'train_loss': 5.386596337873534, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\n",
        "trainer.save_model(\"./KantaiBERT\")"
      ],
      "metadata": {
        "id": "WXljoHERysvp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 15: Language Modeling with the FillMaskPipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./KantaiBERT\",\n",
        "    tokenizer=\"./KantaiBERT\"\n",
        ")"
      ],
      "metadata": {
        "id": "3cLwTlFEysrq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"Human thinking involves human <mask>.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r7DNsyb4bvY",
        "outputId": "782e444e-9393-405d-9fc1-28beb894d446"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.04575687274336815,\n",
              "  'token': 394,\n",
              "  'token_str': ' reason',\n",
              "  'sequence': 'Human thinking involves human reason.'},\n",
              " {'score': 0.02509239688515663,\n",
              "  'token': 421,\n",
              "  'token_str': ' conception',\n",
              "  'sequence': 'Human thinking involves human conception.'},\n",
              " {'score': 0.016969574615359306,\n",
              "  'token': 444,\n",
              "  'token_str': ' law',\n",
              "  'sequence': 'Human thinking involves human law.'},\n",
              " {'score': 0.016534477472305298,\n",
              "  'token': 613,\n",
              "  'token_str': ' principle',\n",
              "  'sequence': 'Human thinking involves human principle.'},\n",
              " {'score': 0.014782223850488663,\n",
              "  'token': 477,\n",
              "  'token_str': ' will',\n",
              "  'sequence': 'Human thinking involves human will.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(\"The critique of pure reason is <mask> <mask>.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40WJ06_0mVoL",
        "outputId": "81cb17f4-479f-4f6d-ff77-a5c3d5447fe2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'score': 0.053565531969070435,\n",
              "   'token': 339,\n",
              "   'token_str': ' not',\n",
              "   'sequence': '<s>The critique of pure reason is not<mask>.</s>'},\n",
              "  {'score': 0.034328099340200424,\n",
              "   'token': 267,\n",
              "   'token_str': ' the',\n",
              "   'sequence': '<s>The critique of pure reason is the<mask>.</s>'},\n",
              "  {'score': 0.03222261369228363,\n",
              "   'token': 289,\n",
              "   'token_str': ' to',\n",
              "   'sequence': '<s>The critique of pure reason is to<mask>.</s>'},\n",
              "  {'score': 0.017299940809607506,\n",
              "   'token': 420,\n",
              "   'token_str': ' only',\n",
              "   'sequence': '<s>The critique of pure reason is only<mask>.</s>'},\n",
              "  {'score': 0.017247438430786133,\n",
              "   'token': 263,\n",
              "   'token_str': ' a',\n",
              "   'sequence': '<s>The critique of pure reason is a<mask>.</s>'}],\n",
              " [{'score': 0.022538399323821068,\n",
              "   'token': 536,\n",
              "   'token_str': ' experience',\n",
              "   'sequence': '<s>The critique of pure reason is<mask> experience.</s>'},\n",
              "  {'score': 0.014826348051428795,\n",
              "   'token': 506,\n",
              "   'token_str': ' itself',\n",
              "   'sequence': '<s>The critique of pure reason is<mask> itself.</s>'},\n",
              "  {'score': 0.012800374999642372,\n",
              "   'token': 571,\n",
              "   'token_str': ' possible',\n",
              "   'sequence': '<s>The critique of pure reason is<mask> possible.</s>'},\n",
              "  {'score': 0.010972971096634865,\n",
              "   'token': 394,\n",
              "   'token_str': ' reason',\n",
              "   'sequence': '<s>The critique of pure reason is<mask> reason.</s>'},\n",
              "  {'score': 0.01092881802469492,\n",
              "   'token': 421,\n",
              "   'token_str': ' conception',\n",
              "   'sequence': '<s>The critique of pure reason is<mask> conception.</s>'}]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKfo3_OKs1Rh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}